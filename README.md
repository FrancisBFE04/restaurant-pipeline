## Restaurant Pipeline 



A production-grade **Kedro-based data pipeline** that ingests restaurant data from several sources (local CSVs + Azure Blob Storage), cleans and transforms it using a bronze-silver-gold layered architecture, and generates business analytics models. This project demonstrates a data pipeline for a restaurant that was constructed using [Kedro](https://kedro.org/).



## Overview 


This pipeline showcases best practices for: The pipeline receives raw data from two CSV files ('raw_customers.csv' and 'raw_orders.csv') as well as Azure Blob Storage (a JSONL file containing support tickets).  It then cleans and transforms the data to create two final datasets: one with the average order value and one with the number of tickets per order.  The pipeline follows the bronze-silver-gold architecture:

- **Multi-source data ingestion** (local files + cloud blob storage)

- **Layered architecture** (bronze â†’ silver â†’ gold)*   **Bronze:** Raw, unprocessed data.

- **Data quality** (deduplication, null handling, type validation)*   **Silver:** Cleaned and transformed data.

- **Reproducibility** (Kedro framework with versioned configs)*   **Gold:** Final, aggregated data ready for reporting or analysis.

- **Analytics models** (average order value, support ticket metrics, revenue)

## Project Structure

### Key Features

Ingests 4 local CSV files (customers, orders, products, items)  The project is structured as a standard Kedro project:

Connects to Azure Blob Storage for support tickets (JSONL format)  

Cleans nested JSONL objects without data loss  *   `conf/`: Configuration files, including the data catalog (`catalog.yml`).

Generates 3 gold-layer analytics CSVs  *   `data/`: Data files, organized into `bronze`, `silver`, and `gold` layers.

Fully tested with pytest fixtures  *   `src/`: Source code, including the pipeline and node definitions.

Production-ready code with inline documentation  

## How to run the pipeline

---

To run this pipeline, you need to have Python 3.10+ and `pip` installed.

## Architecture

1.  **Clone the repository:**

### Data Flow

    ```bash

```    git clone <repository-url>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    cd <repository-name>

â”‚           BRONZE LAYER (Raw Ingestion)                  â”‚    ```

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

â”‚                                                         â”‚2.  **Create a virtual environment:**

â”‚  ğŸ“‚ Local CSVs              ğŸŒ Azure Blob Storage       â”‚

â”‚  â”œâ”€ raw_customers.csv       â””â”€ support_tickets.jsonl   â”‚    ```bash

â”‚  â”œâ”€ raw_orders.csv              (SAS URL)              â”‚    python -m venv venv

â”‚  â”œâ”€ raw_products.csv                                   â”‚    ```

â”‚  â””â”€ raw_items.csv                                      â”‚

â”‚                                                         â”‚3.  **Activate the virtual environment:**

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚ clean_*_node    *   On Windows:

                       â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        ```bash

â”‚         SILVER LAYER (Cleaned & Standardized)           â”‚        .\venv\Scripts\activate

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        ```

â”‚                                                         â”‚

â”‚  ğŸ“¦ Deduplicated Parquet Files                          â”‚    *   On macOS and Linux:

â”‚  â”œâ”€ cleaned_customers.parquet                          â”‚

â”‚  â”œâ”€ cleaned_orders.parquet  (order_id renamed)        â”‚        ```bash

â”‚  â””â”€ cleaned_tickets.parquet (nested fields serialized) â”‚        source venv/bin/activate

â”‚                                                         â”‚        ```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚ create_*_node4.  **Install the dependencies:**

                       â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ```bash

â”‚        GOLD LAYER (Analytics & Reporting)              â”‚    pip install -r requirements.txt

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    ```

â”‚                                                         â”‚

â”‚  ğŸ“Š Consumer-Ready CSV Files                            â”‚5.  **Run the pipeline:**

â”‚  â”œâ”€ average_order_value.csv    ($993.17)              â”‚

â”‚  â”œâ”€ tickets_per_order.csv      (2.5M rows)            â”‚    ```bash

â”‚  â””â”€ total_revenue.csv          ($62.7M)               â”‚    kedro run

â”‚                                                         â”‚    ```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```6.  **Check the results:**



### Directory Structure    The final output will be in the `data/gold` directory. You will find two files: `average_order_value.csv` and `tickets_per_order.csv`.


---

## Quick Start

### Prerequisites

- **Python 3.9+** (tested on 3.13)
- **Pip** or **Conda**

### Installation

1. **Navigate to project:**
   ```bash
   cd restaurant-pipeline
   ```

2. **Create and activate virtual environment:**
   ```bash
   python -m venv venv
   venv\Scripts\activate  # Windows
   source venv/bin/activate  # macOS/Linux
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

### Running the Pipeline

**Full pipeline (all 6 nodes):**
```bash
kedro run
```

**Data processing only (bronze â†’ silver):**
```bash
kedro run --pipeline data_processing
```

**Reporting only (silver â†’ gold):**
```bash
kedro run --pipeline reporting
```

**From specific node:**
```bash
kedro run --from-nodes clean_orders_node
```

### Running Tests

```bash
pytest
pytest --cov=src/restaurant_pipeline tests/  # with coverage
```

---

## Data Sources

### Local CSV Files (Bronze Layer)

| File | Rows |
|------|------|
| `raw_customers.csv` | Customer master data |
| `raw_orders.csv` | Transaction records |
| `raw_products.csv` | Product catalog |
| `raw_items.csv` | Order line items |

### Azure Blob Storage (JSONL)

- **URL:**  Enter your SAS URL here
- **Format:** JSON Lines (one JSON object per line)
- **Rows:** ~90K support tickets
- **Auth:** SAS token

**Sample record:**
```json
{
  "ticket_id": "TCK-317F60A0AA",
  "order_id": "0000dda0-bedb-4109-bdfb-1bbbed16af12",
  "tags": ["order", "delivery"],
  "subject": "Delivery Status",
  "status": "resolved"
}
```

---

## Data Cleaning Logic

### Bronze â†’ Silver Transformations

#### `clean_customers`
- Remove duplicate rows
- Remove rows with null values
- **Output:** `cleaned_customers.parquet`

#### `clean_orders`
- **Rename:** `id` â†’ `order_id` (for consistency with tickets join key)
- Remove duplicates
- Remove nulls
- **Parse:** `ordered_at` to datetime (enables temporal analysis)
- **Output:** `cleaned_orders.parquet`

#### `clean_tickets`
- **Challenge:** JSONL contains nested dict/list values (e.g., `tags: ["order", "delivery"]`)
  - pandas.drop_duplicates() fails: `TypeError: unhashable type: 'dict'`
- **Solution:** Serialize dict/list to JSON strings before deduplication
  - Use helper `_safe_serialize()` with `sort_keys=True` for deterministic serialization
- Remove duplicates
- Remove nulls
- **Output:** `cleaned_tickets.parquet`

---

## Analytics Models (Gold Layer)

### 1. Average Order Value
**File:** `data/gold/average_order_value.csv`  
**Formula:** `mean(orders.subtotal)`  
**Value:** $993.17  
**Use Case:** Revenue per transaction KPI

### 2. Tickets per Order
**File:** `data/gold/tickets_per_order.csv`  
**Formula:** Join orders â†” tickets on order_id, count tickets per order  
**Rows:** ~2.5M  
**Use Case:** Support load analysis

### 3. Total Revenue
**File:** `data/gold/total_revenue.csv`  
**Formula:** `sum(orders.subtotal)`  
**Value:** $62,716,500  
**Use Case:** Financial reporting

---

## Configuration

### `conf/base/catalog.yml`

Defines all datasets (sources and outputs):

```yaml
raw_customers:
  type: pandas.CSVDataset
  filepath: data/bronze/raw_customers.csv

tickets_jsonl:
  type: pandas.JSONDataset
  filepath: Your filepath here
  load_args:
    lines: True  # JSONL format

cleaned_customers:
  type: pandas.ParquetDataset
  filepath: data/silver/cleaned_customers.parquet

average_order_value:
  type: pandas.CSVDataset
  filepath: data/gold/average_order_value.csv
```

### `conf/base/parameters.yml`

Pipeline parameters (extensible for data quality rules, thresholds, etc.)

### `conf/local/credentials.yml`

Git-ignored local secrets (currently unused; Azure auth via SAS token in URL)

---

## Testing

### Test Structure

```
tests/
â”œâ”€â”€ conftest.py                    # Shared fixtures & mock data
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ test_data_processing.py   # Test clean_* nodes
â”‚   â””â”€â”€ test_reporting.py         # Test create_* nodes
```

### Running Tests

```bash
pytest  # All tests
pytest tests/pipelines/test_data_processing.py -v  # Specific module, verbose
pytest --cov=src/restaurant_pipeline tests/  # With coverage report
```
---
##  Final Project Structure
```
restaurant-pipeline/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ pyproject.toml                      # Project metadata & dependencies
â”œâ”€â”€ requirements.txt                    # Python packages
â”œâ”€â”€ pytest.ini                          # Pytest configuration
â”‚
â”œâ”€â”€ conf/                               # Configuration (non-code)
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ catalog.yml                 # Data sources & outputs
â”‚   â”‚   â””â”€â”€ parameters.yml              # Pipeline parameters
â”‚   â””â”€â”€ local/
â”‚       â””â”€â”€ credentials.yml             # Local secrets (git-ignored)
â”‚
â”œâ”€â”€ data/                               # Data directory
â”‚   â”œâ”€â”€ bronze/                         # Raw data (ingestion sources)
â”‚   â”‚   â”œâ”€â”€ raw_customers.csv
â”‚   â”‚   â”œâ”€â”€ raw_orders.csv
â”‚   â”‚   â”œâ”€â”€ raw_products.csv
â”‚   â”‚   â””â”€â”€ raw_items.csv
â”‚   â”œâ”€â”€ silver/                         # Cleaned data (intermediates)
â”‚   â”‚   â”œâ”€â”€ cleaned_customers.parquet
â”‚   â”‚   â”œâ”€â”€ cleaned_orders.parquet
â”‚   â”‚   â””â”€â”€ cleaned_tickets.parquet
â”‚   â””â”€â”€ gold/                           # Analytics output (final)
â”‚       â”œâ”€â”€ average_order_value.csv
â”‚       â”œâ”€â”€ tickets_per_order.csv
â”‚       â””â”€â”€ total_revenue.csv
â”‚
â”œâ”€â”€ src/restaurant_pipeline/            # Main source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py
â”‚   â”œâ”€â”€ pipeline_registry.py            # Kedro pipeline registry
â”‚   â”œâ”€â”€ settings.py                     # Kedro configuration
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ data_processing/            # Bronze â†’ Silver layer
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ nodes.py               # clean_customers, clean_orders, clean_tickets
â”‚       â”‚   â””â”€â”€ pipeline.py            # Data processing DAG
â”‚       â””â”€â”€ reporting/                 # Silver â†’ Gold layer
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ nodes.py               # Average value, tickets/order, revenue
â”‚           â””â”€â”€ pipeline.py            # Reporting DAG
â”‚
â”œâ”€â”€ tests/                              # Unit & integration tests
â”‚   â”œâ”€â”€ conftest.py                    # Pytest fixtures & setup
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ test_data_processing.py    # Test data_processing nodes
â”‚   â”‚   â””â”€â”€ test_reporting.py          # Test reporting nodes
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ docs/                               # Documentation
    â””â”€â”€ ARCHITECTURE.md                 # Extended design docs
```
---
